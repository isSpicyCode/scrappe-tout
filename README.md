# Scrappe-Tout

[![French Version](https://img.shields.io/badge/ğŸ‡«ğŸ‡·_Version-FranÃ§aise-blue)](./README_fr.md)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18-brightgreen)](https://nodejs.org/)
[![npm version](https://img.shields.io/github/package-json/v/isSpicyCode/scrappe-tout)](https://github.com/isSpicyCode/scrappe-tout)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen)](./tests/)

Ultra-fast web scraper with Playwright that converts HTML pages to clean Markdown format. Designed for documentation archiving and RAG (Retrieval-Augmented Generation) workflows.

**Works seamlessly in:**
- Interactive terminals (bash, zsh, fish, etc.) with TUI menu
- Claude Code / AI coding assistants (auto-detects non-interactive mode)
- CI/CD pipelines and automated workflows

## Features

- **High Performance**: Averages 0.8-1.2 seconds per URL using Playwright
- **Clean Output**: Removes navigation menus, duplicated tables of contents, and unnecessary elements
- **Smart Filenames**: Generates short, readable filenames from URL paths
- **Hybrid Mode**: Automatically detects interactive terminal vs non-interactive environments
- **Resource Blocking**: Blocks 10+ resource patterns (ads, analytics, tracking) for faster scraping
- **Markdown Optimization**: Single table of contents preserved for RAG applications
- **Batch Processing**: Process multiple URLs sequentially with progress tracking

## Requirements

- **Node.js 18+** - [Download from nodejs.org](https://nodejs.org/) or install via package manager:
  ```bash
  # Ubuntu/Debian
  curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
  sudo apt-get install -y nodejs
  
  # macOS (with Homebrew)
  brew install node
  
  # Or use nvm (recommended)
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
  nvm install 18
  ```
- npm or yarn (included with Node.js)

## Installation

```bash
# Clone the repository
git clone https://github.com/isSpicyCode/scrappe-tout.git
cd scrappe-tout

# Install dependencies
npm install

# Install Playwright browsers
npx playwright install chromium
```

## Usage

### Quick Start

1. Add your URLs to scrape in the `urls.txt` file (one URL per line):
```
https://example.com/docs/getting-started
https://example.com/docs/installation
https://example.com/docs/architecture
```

2. Run the scraper:
```bash
npm start
```

**Progress Display:**
```
[1/30] [100%] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] https://example.com/docs/getting-started (1s)
[2/30] [100%] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] https://example.com/docs/installation (1s)
[3/30] [100%] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] https://example.com/docs/architecture (1s)
```
Each URL shows its individual progress bar reaching 100%, then moves to the next line.

### Interactive Mode (Terminal)

When running in an interactive terminal, the behavior depends on whether a `scrap-folder-name.txt` file exists:

**If `scrap-folder-name.txt` exists:**
- The folder name from the file is used automatically
- The menu is bypassed

**If no `scrap-folder-name.txt` file:**
- A TUI menu appears for folder selection:

```
============================================================
  FOLDER MENU - Select an option
============================================================
  Existing folders:
    1. my-files-docs
    2. scrap [default]

  Options:
    N - Create new folder (default: "scrap")
    D - Use default folder
    X - Delete existing folder
    Q - Quit
============================================================
Your choice:
```

Options:
- **N**: Create a new folder with custom name
- **D**: Use default "scrap" folder
- **1, 2, ...**: Use existing folder
- **X**: Delete an existing folder
- **Q**: Quit

### Non-Interactive Mode (Claude Code, CI)

In non-interactive environments (Claude Code, CI/CD), the folder name is determined automatically:

**Priority order:**
1. **`scrap-folder-name.txt` file** (if exists)
   ```bash
   # Create folder name file
   echo "my-documentation" > scrap-folder-name.txt
   
   # Run scraper
   npm start
   ```

2. **Auto-generated timestamp** (if no file exists)
   - Automatically creates a timestamped folder (e.g., `scrap-2026-02-13T10-49-30`)
   - Ensures no conflicts between runs

### Folder Name Priority

The scraper determines the folder name in this order:
1. ğŸ¥‡ **`--name` flag** (if provided) - Highest priority
2. ğŸ¥ˆ **`scrap-folder-name.txt` file** (if exists) - Works in both interactive and non-interactive modes
3. ğŸ¥‰ **Interactive menu** (if terminal is interactive AND no file exists)
4. â° **Timestamp** (fallback if nothing else is available)

### Command-Line Options

```bash
# Specify custom output directory
npm start -- --output-dir /path/to/output

# Show help
npm start -- --help
```

## Output Structure

```
captures/
â”œâ”€â”€ my-files-docs/
â”‚   â”œâ”€â”€ inspector.md
â”‚   â”œâ”€â”€ memory.md
â”‚   â”œâ”€â”€ performance.md
â”‚   â””â”€â”€ ...
â””â”€â”€ scrap-2026-02-13T10-49-30/
    â”œâ”€â”€ ui.md
    â”œâ”€â”€ components.md
    â””â”€â”€ ...
```

Each URL generates a single Markdown file with:
- Clean content (no navigation, ads, or clutter)
- Preserved table of contents for RAG applications
- Short filename based on the last URL path segment

## Configuration

### Resource Blocking

The scraper automatically blocks these resource patterns for faster loading:
- Analytics and tracking scripts
- Advertisement networks
- Social media widgets
- Fonts and stylesheets from CDNs
- Images (can be enabled in config)

### Customize Blocked Resources

Edit `src/core/scraper.js` to modify the `RESOURCE_PATTERNS` array.

## Performance

| Metric | Value |
|--------|-------|
| Average per URL | 0.8-1.2 seconds |
| HTML compression | 80-95% size reduction |
| Conversion speed | 10-20x faster than single-file scripts |
| Parallel processing | Sequential (prevents rate limiting) |

## Project Structure

```
scrappe-tout/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ scraper.js          # Playwright scraping logic
â”‚   â”‚   â”œâ”€â”€ converter.js        # HTML to Markdown conversion
â”‚   â”‚   â”œâ”€â”€ writer.js           # File writing with smart naming
â”‚   â”‚   â”œâ”€â”€ postprocessor.js    # Content cleaning
â”‚   â”‚   â”œâ”€â”€ logger.js           # Logging utilities
â”‚   â”‚   â””â”€â”€ navigation-cleaner.js   # Navigation patterns and removal
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ config.js           # Configuration management
â”‚   â”‚   â”œâ”€â”€ retry.js            # Retry logic with exponential backoff
â”‚   â”‚   â”œâ”€â”€ error.js            # Error handling
â”‚   â”‚   â”œâ”€â”€ urls.js             # URL reading and validation
â”‚   â”‚   â”œâ”€â”€ pipeline.js         # Scraping pipeline orchestration
â”‚   â”‚   â””â”€â”€ path.js             # Output directory management
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ cli.js              # Command-line argument parsing
â”‚   â”‚   â”œâ”€â”€ menu.js             # Interactive TUI menu
â”‚   â”‚   â”œâ”€â”€ display.js          # Duration formatting and progress bar
â”‚   â”‚   â”œâ”€â”€ stats.js            # Statistics generation
â”‚   â”‚   â”œâ”€â”€ timestamp.js        # Timestamp utilities
â”‚   â”‚   â””â”€â”€ constants.js        # Application constants
â”‚   â””â”€â”€ index.js                 # Main entry point (orchestration only)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ e2e/
â”‚   â”‚   â””â”€â”€ scraping.test.js     # End-to-end workflow tests
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ cli.test.js         # CLI parser tests
â”‚   â”‚   â”œâ”€â”€ display.test.js     # Display utilities tests
â”‚   â”‚   â””â”€â”€ stats.test.js       # Statistics tests
â”‚   â””â”€â”€ fixtures/
â”‚       â””â”€â”€ test-urls.txt       # Sample URLs for testing
â”œâ”€â”€ urls.txt                     # URLs to scrape (one per line)
â”œâ”€â”€ scrap-folder-name.txt        # Custom output folder name
â””â”€â”€ package.json
```

## Acknowledgments

Built with:
- [Playwright](https://playwright.dev/) - Fast and reliable web automation
- [mdream](https://www.npmjs.com/package/mdream) - HTML to Markdown conversion
- [Biome](https://biomejs.dev/) - Linting and formatting

## Troubleshooting

### Playwright browsers not installed
```bash
npx playwright install chromium
```

### Permission errors on Linux
```bash
# Install required dependencies
sudo npx playwright install-deps chromium
```

### Empty or failed scrapes
- Check that URLs in `urls.txt` are accessible
- Some sites may require authentication or block automated access
- Try adding delays or reducing concurrency for rate-limited sites

### Module not found errors
```bash
# Reinstall dependencies
rm -rf node_modules package-lock.json
npm install
```

## License

MIT License - see LICENSE file for details.

Copyright (c) 2026 Spicycode - Scrappe-Tout Contributors
