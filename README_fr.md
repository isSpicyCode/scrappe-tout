# Scrappe-Tout

[![English Version](https://img.shields.io/badge/ğŸ‡¬ğŸ‡§_Version-English-red)](./README.md)
[![Licence: MIT](https://img.shields.io/badge/Licence-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version Node.js](https://img.shields.io/badge/node-%3E%3D18-brightgreen)](https://nodejs.org/)
[![npm version](https://img.shields.io/github/package-json/v/isSpicyCode/scrappe-tout)](https://github.com/isSpicyCode/scrappe-tout)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen)](./tests/)

Scraper web ultra-rapide avec Playwright qui convertit les pages HTML en format Markdown propre. ConÃ§u pour l'archivage de documentation et les workflows RAG (Retrieval-Augmented Generation).

**Fonctionne parfaitement dans :**
- Terminaux interactifs (bash, zsh, fish, etc.) avec menu TUI
- Claude Code / assistants de codage IA (dÃ©tection automatique du mode non-interactif)
- Pipelines CI/CD et workflows automatisÃ©s

## FonctionnalitÃ©s

- **Haute Performance** : Moyenne de 0,8-1,2 secondes par URL avec Playwright
- **Sortie Propre** : Supprime les menus de navigation, tables des matiÃ¨res dupliquÃ©es et Ã©lÃ©ments inutiles
- **Noms de Fichiers Intelligents** : GÃ©nÃ¨re des noms de fichiers courts et lisibles Ã  partir des chemins d'URL
- **Mode Hybride** : DÃ©tection automatique du mode terminal interactif vs non-interactif
- **Blocage de Ressources** : Bloque plus de 10 patterns de ressources (pubs, analytics, tracking) pour un scraping plus rapide
- **Optimisation Markdown** : Table des matiÃ¨res unique prÃ©servÃ©e pour les applications RAG
- **Traitement par Lots** : Traite plusieurs URLs sÃ©quentiellement avec suivi de progression

## PrÃ©requis

- **Node.js 18+** - [TÃ©lÃ©charger depuis nodejs.org](https://nodejs.org/) ou installer via gestionnaire de paquets :
  ```bash
  # Ubuntu/Debian
  curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
  sudo apt-get install -y nodejs
  
  # macOS (avec Homebrew)
  brew install node
  
  # Ou utiliser nvm (recommandÃ©)
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
  nvm install 18
  ```
- npm ou yarn (inclus avec Node.js)

## Installation

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/isSpicyCode/scrappe-tout.git
cd scrappe-tout

# Installer les dÃ©pendances
npm install

# Installer les navigateurs Playwright
npx playwright install chromium
```

## Utilisation

### DÃ©marrage Rapide

1. Ajoutez vos URLs Ã  scraper dans le fichier `urls.txt` (une URL par ligne) :
```
https://example.com/docs/getting-started
https://example.com/docs/installation
https://example.com/docs/architecture
```

2. Lancez le scraper :
```bash
npm start
```

**Affichage de la progression :**
```
[1/30] [100%] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] https://example.com/docs/getting-started (1s)
[2/30] [100%] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] https://example.com/docs/installation (1s)
[3/30] [100%] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] https://example.com/docs/architecture (1s)
```
Chaque URL affiche sa propre barre de progression atteignant 100%, puis passe Ã  la ligne suivante.

### Mode Interactif (Terminal)

Lors de l'exÃ©cution dans un terminal interactif, le comportement dÃ©pend de l'existence d'un fichier `scrap-folder-name.txt` :

**Si `scrap-folder-name.txt` existe :**
- Le nom du dossier depuis le fichier est utilisÃ© automatiquement
- Le menu est contournÃ©

**Si pas de fichier `scrap-folder-name.txt` :**
- Un menu TUI apparaÃ®t pour la sÃ©lection du dossier :

```
============================================================
  FOLDER MENU - Select an option
============================================================
  Existing folders:
    1. my-files-docs
    2. scrap [default]

  Options:
    N - Create new folder (default: "scrap")
    D - Use default folder
    X - Delete existing folder
    Q - Quit
============================================================
Your choice:
```

Options :
- **N** : CrÃ©er un nouveau dossier avec nom personnalisÃ©
- **D** : Utiliser le dossier par dÃ©faut "scrap"
- **1, 2, ...** : Utiliser un dossier existant
- **X** : Supprimer un dossier existant
- **Q** : Quitter

### Mode Non-Interactif (Claude Code, CI)

Dans les environnements non-interactifs (Claude Code, CI/CD), le nom du dossier est dÃ©terminÃ© automatiquement :

**Ordre de prioritÃ© :**
1. **Fichier `scrap-folder-name.txt`** (s'il existe)
   ```bash
   # CrÃ©er le fichier avec le nom du dossier
   echo "my-documentation" > scrap-folder-name.txt
   
   # Lancer le scraper
   npm start
   ```

2. **Timestamp automatique** (si aucun fichier n'existe)
   - CrÃ©e automatiquement un dossier horodatÃ© (ex: `scrap-2026-02-13T10-49-30`)
   - Garantit l'absence de conflits entre les exÃ©cutions

### PrioritÃ© du Nom de Dossier

Le scraper dÃ©termine le nom du dossier dans cet ordre :
1. ğŸ¥‡ **Flag `--name`** (si fourni) - PrioritÃ© la plus haute
2. ğŸ¥ˆ **Fichier `scrap-folder-name.txt`** (s'il existe) - Fonctionne en mode interactif et non-interactif
3. ğŸ¥‰ **Menu interactif** (si le terminal est interactif ET qu'aucun fichier n'existe)
4. â° **Timestamp** (solution de repli si rien d'autre n'est disponible)

### Options en Ligne de Commande

```bash
# SpÃ©cifier un rÃ©pertoire de sortie personnalisÃ©
npm start -- --output-dir /chemin/vers/sortie

# Afficher l'aide
npm start -- --help
```

## Structure de Sortie

```
captures/
â”œâ”€â”€ my-files-docs/
â”‚   â”œâ”€â”€ inspector.md
â”‚   â”œâ”€â”€ memory.md
â”‚   â”œâ”€â”€ performance.md
â”‚   â””â”€â”€ ...
â””â”€â”€ scrap-2026-02-13T10-49-30/
    â”œâ”€â”€ ui.md
    â”œâ”€â”€ components.md
    â””â”€â”€ ...
```

Chaque URL gÃ©nÃ¨re un seul fichier Markdown avec :
- Contenu propre (sans navigation, publicitÃ©s ou encombrement)
- Table des matiÃ¨res prÃ©servÃ©e pour les applications RAG
- Nom de fichier court basÃ© sur le dernier segment du chemin URL

## Configuration

### Blocage de Ressources

Le scraper bloque automatiquement ces patterns de ressources pour un chargement plus rapide :
- Scripts d'analytics et de tracking
- RÃ©seaux publicitaires
- Widgets de rÃ©seaux sociaux
- Polices et feuilles de style depuis les CDN
- Images (peut Ãªtre activÃ© dans la config)

### Personnaliser les Ressources BloquÃ©es

Ã‰ditez `src/core/scraper.js` pour modifier le tableau `RESOURCE_PATTERNS`.

## Performance

| MÃ©trique | Valeur |
|----------|--------|
| Moyenne par URL | 0,8-1,2 secondes |
| Compression HTML | RÃ©duction de taille de 80-95% |
| Vitesse de conversion | 10-20x plus rapide que les scripts Ã  fichier unique |
| Traitement parallÃ¨le | SÃ©quentiel (prÃ©vient la limitation de dÃ©bit) |

## Structure du Projet

```
scrappe-tout/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ scraper.js          # Logique de scraping Playwright
â”‚   â”‚   â”œâ”€â”€ converter.js        # Conversion HTML vers Markdown
â”‚   â”‚   â”œâ”€â”€ writer.js           # Ã‰criture de fichiers avec nommage intelligent
â”‚   â”‚   â”œâ”€â”€ postprocessor.js    # Nettoyage de contenu
â”‚   â”‚   â”œâ”€â”€ logger.js           # Utilitaires de logging
â”‚   â”‚   â””â”€â”€ navigation-cleaner.js   # Patterns de navigation et suppression
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ config.js           # Gestion de configuration
â”‚   â”‚   â”œâ”€â”€ retry.js            # Logique de retry avec backoff exponentiel
â”‚   â”‚   â”œâ”€â”€ error.js            # Gestion d'erreurs
â”‚   â”‚   â”œâ”€â”€ urls.js             # Lecture et validation d'URLs
â”‚   â”‚   â”œâ”€â”€ pipeline.js         # Orchestration du pipeline de scraping
â”‚   â”‚   â””â”€â”€ path.js             # Gestion des rÃ©pertoires de sortie
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ cli.js              # Parsing d'arguments en ligne de commande
â”‚   â”‚   â”œâ”€â”€ menu.js             # Menu TUI interactif
â”‚   â”‚   â”œâ”€â”€ display.js          # Formatage de durÃ©e et barre de progression
â”‚   â”‚   â”œâ”€â”€ stats.js            # GÃ©nÃ©ration de statistiques
â”‚   â”‚   â”œâ”€â”€ timestamp.js        # Utilitaires de timestamp
â”‚   â”‚   â””â”€â”€ constants.js        # Constantes de l'application
â”‚   â””â”€â”€ index.js                 # Point d'entrÃ©e principal (orchestration uniquement)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ e2e/
â”‚   â”‚   â””â”€â”€ scraping.test.js     # Tests de workflow end-to-end
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ cli.test.js         # Tests du parseur CLI
â”‚   â”‚   â”œâ”€â”€ display.test.js     # Tests des utilitaires d'affichage
â”‚   â”‚   â””â”€â”€ stats.test.js       # Tests des statistiques
â”‚   â””â”€â”€ fixtures/
â”‚       â””â”€â”€ test-urls.txt       # Exemples d'URLs pour les tests
â”œâ”€â”€ urls.txt                     # URLs Ã  scraper (une par ligne)
â”œâ”€â”€ scrap-folder-name.txt        # Nom de dossier de sortie personnalisÃ©
â””â”€â”€ package.json
```

## Remerciements

Construit avec :
- [Playwright](https://playwright.dev/) - Automatisation web rapide et fiable
- [mdream](https://www.npmjs.com/package/mdream) - Conversion HTML vers Markdown
- [Biome](https://biomejs.dev/) - Linting et formatage

## DÃ©pannage

### Navigateurs Playwright non installÃ©s
```bash
npx playwright install chromium
```

### Erreurs de permissions sur Linux
```bash
# Installer les dÃ©pendances requises
sudo npx playwright install-deps chromium
```

### Scraping vide ou Ã©chouÃ©
- VÃ©rifiez que les URLs dans `urls.txt` sont accessibles
- Certains sites peuvent nÃ©cessiter une authentification ou bloquer l'accÃ¨s automatisÃ©
- Essayez d'ajouter des dÃ©lais ou de rÃ©duire la concurrence pour les sites avec limitation de dÃ©bit

### Erreurs de modules non trouvÃ©s
```bash
# RÃ©installer les dÃ©pendances
rm -rf node_modules package-lock.json
npm install
```

## Licence

Licence MIT - voir le fichier LICENSE pour les dÃ©tails.

Copyright (c) 2026 Spicycode - Contributeurs Scrappe-Tout
